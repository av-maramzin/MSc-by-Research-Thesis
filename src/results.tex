\chapter{Results}
\label{results}
\qquad This chapter summarizes the main results of the undertaken MSc by Research 2018 project and describes its workflow as it happened. In short, the main results of the undertaken MSc by Research project can be described with several points:
\begin{enumerate}[align=left,leftmargin=*]
\item \textbf{Software source code parallelizability metrics search has been conducted. MSc bt Research project has been set up.}\newline
\null\qquad A body of literature has been searched through in an attempt to find any software source code metrics, applicable to the software parallelizability problem. While there are a lot of metrics aimed at judging about software quality (maintainability, readability, etc.), none were proposed to judge about software parallelizability. The only metrics in the subfield of parallel computation represent different variations of program execution time speedup ratio. Short report is given in the sections \ref{background-software-metrics-in-cs} and \ref{background-metrics-parallel-computing} of the background chapter \ref{backgroud}.\newline
\null\qquad It was decided, that Program Dependence Graph (PDG) (proposed in the paper \cite{pdg-paper}) is going to be an intermediate program representation, parallelizability metrics would be computed on. Moreover, parallelizability metrics would use loop decoupling and loop iterator recognition results (proposed in the paper \cite{iterator-recognition-paper}) as a prerequisite for their further computation. The first metric to be computed was decided to be \textit{loop payload fraction} (see \ref{metrics-loop-payload-fraction}). LLVM compiler components library \cite{llvm-official-website},\cite{llvm-paper} was chosen for the MSc project to be based on.	
\item \textbf{Development of LLVM-based metric computing tool.}\newline 
\null\qquad This stage took the most of the time and efforts. As a result PPar tool (described in \ref{ppar-tool} and hosted on the GitHub \cite{ppar-tool}) ($\approx$ 4750 C/C++ lines of code) has been developed. The tool development started straight after the first metric ideas were conceived (end of January 2018).\newline
\null\qquad The PDG intermediate representation and all algorithms, proposed and described in \cite{pdg-paper} and \cite{iterator-recognition-paper} have been implemented from the scratch. This took a significant project start-up overheads. Development of dependence graph intermediate representation on top of LLVM IR along with loop decoupling and iterator recognition algorithms (strongly connected components search) took more than a month of time. During that period LLVM DEBUG() prints served as the only debugging and validation means.\newline
\null\qquad After the first graph visualization facilities (thanks to Graphviz and DOT) had been added the project, the first research work has actually started. Along with further validation, debugging and development of the tool, first metrics started to appear on the small hand-written tests. Metric values have been manually validated against PDG and its SCCs DOT graph visualizations. Visualizations of these PDG and their SCCs served as an inspiration for the proposal of a new metrics.\newline
\null\qquad By June 2018, 17 metrics have been devised and integrated into the developed PPar tool framework. Metric values could be obtained on the small set of hand-written tests. These values have been manually verified with graph visualizations. This work has been reported during the Intermediate Progress Review held on the 7$^{th}$ of June 2018.                    
\item \textbf{Devised metric values have been collected on the NAS benchmark suite.}
	
\item \textbf{Analysis of devised parallelizability metrics.} Once all metric values had been gathered for all NAS benchmark loops and all loops had been classified with Intel C/C++ compiler, the final stage of the project could be started.     
	
\end{enumerate}





\qquad The overall picture in the matter of software source code (loops in particular) metrics for  parallelisability resembeles that of the software quality metrics. Software quality (say, maintenance) is a complex notion. To judge about good or bad software design one must posses vast software engeeniring expertise and skills. Metrics like cyclomatic complexity can be used as supplementary to manual analysis, but the values they give must be examined by a human with a deep understanding of the software quality question. Although, their values might sometimes correlate with the understanding of a sound software design, these metrics should not be applied blindly.\newline
\null\qquad Software parallellizability property is not a simplier one. Despite the fact, that all examined metrics have grounds to be proposed and are not randomly selected, their parallelizability correlations are limited and there are always special cases, which break generally established rules and patterns.\newline


\null\qquad However, the working framework, developed withing that  MSc by research project, is ready and can be used for further metrics research and analysis. It is quite easy to add new metrics to the tool. Tool provides visualization facilities for dependence graphs and loop iterator/payload decomposition. New metrics might be added. Alternatively, existing metrics might be fine-tuned as well. This work might be the one on the relatively new direction. Application of machine learning in compilers. Since all machine learning methods require some quantitative features, these loop metrics might be an attempt in their establishment. Modern compilers apply a set of optimizations: loop unrolling, peeling, splitting. Correlations between these metrics and these properties (like loop unrollability) might be examined towards development of machine-learning driven compiler optimizers.\newline
\null\qquad A lot of time has been spent on the development of the tool itself and the first, suitable for analysis, results appeared quite late in the timeframe of the project.         