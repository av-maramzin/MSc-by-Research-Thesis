\chapter{Background} \label{backgroud}
\qquad This chapter of the thesis introduces a reader into the context of the work. First, sections \ref{background-software-metrics-in-cs} and \ref{background-metrics-parallel-computing} give a broad overview of various software source code metrics proposed in the general field of computer science and specifically in the subfield of parallel computing. None of these metrics can be directly used to judge about software source code parallelizability. For tackling that problem there is a need in development of new source code parallelizability metrics.\newline
\null\qquad The field of parallel computing is a really old field, dating back to 1960s and there has been done a huge amount of work. Talking on the very high abstract level, the parallelization of a sequantial program is ultimately constrained by different sorts of dependencies, which exist between program instructions. By that time, there has been established a theory on program dependence analysis. This theory has been well absorbed into many commercial compilers and is being ubiquitously used everywhere in practice. The book \cite{optimizing-compilers-book} conducts a broad compilation of all major results in the field of parallelizing compilers and program dependence theory. Section \ref{background-modern-parallelisability-advisor-tools} briefly describes modern state-of-the-art parallelization tools with Intel's Parallel Studio \cite{intel-parallel-studio} as an example. Section \ref{background-dependence-theory} takes the main theory results from \cite{optimizing-compilers-book} and describes those, which are used in the current project.\newline
\null\qquad This abundance of previous work makes it clear, that there is only one way along which software source code parallelizability metrics study can be conducted. Proposed metrics must use the results of well-established dependence theory. In particular, the whole MSc project work is based on the intermediate dependence-based program representation, namely Program Dependence Graph (PDG). This program dependence data structure has been proposed by in their work \cite{pdg-paper} and is briefly described in section \ref{background-program-dependence-graph}.\newline
\null\qquad Once PDG of a program source code is built, some metrics can be computed straight away. Regular program consists out of modules, subroutines, blocks of code (such as if-statements, for-loops, etc.). As, according to many observations, loops represent 20\% of code, which is executed 80\% of time and are the most seducing piece of workload to parallelize by compiler, this MSc work computes parallelizability metrics on program loops.\newline
\null\qquad The work on loop iterator recognition  and loop decoupling \cite{iterator-recognition-paper} is another foundation this MSc project is based on. Section \ref{background-loop-decoupling} describes the main results of this paper and introduces notions of loop iterator and payload. Metrics proposed and described in chapter \ref{metrics} almost completely rely onto these concepts. The tool, developed for this MSc project (see chapter \ref{ppar-tool}) implements algorithms described in \cite{iterator-recognition-paper} and also uses some graph theory results like strongly connected components (SCCs) finding algorithm and some results from compilers control flow analysis theory, which are briefly outlined in sections \ref{background-graph-theory} and \ref{background-control-flow-analysis}.

\section{Software metrics in computer science} \label{background-software-metrics-in-cs}
\qquad The idea of software source code metrics is definitely not a new one. Quantitative measurements lie as the essence of all exact sciences and there have been numerous efforts to introduce objective metrics in computer science as well. As of the moment computer science quantitative metrics have found their
application mostly in the fields of software quality assessment, software products complexity and software development as a process. These metrics measure
properties of software products such as source code complexity, modularity,
testability and ultimately maintainability. Combined with properties related to
software development processes and projects, they are capable of delivering some
estimates on the total amount of development efforts and associated monetary costs at the end.\newline 
\null\qquad The body of research in this relatively new field is very vast. There are a lot of publications on different types of metrics as well as on their evaluation criteria, axioms the metrics must conform to, their validation, applicability, etc. There has been some efforts to conduct a survey of the field and present an overview of the most important and widespread software metrics to date ([1],[2],[3] to name a few). Work [2] distinguishes two major eras in the field: before 1991, where the main focus was on metrics based on the complexity of the code; and after 1992, where the main focus was on metrics based on the concepts of Object Oriented (OO) systems (design and implementation). Earlier Fabrizio Riguzzi's work [1] dated as 1996 resembles
[2], but also adds some critical insight. Jitender Kumar Chhabra and Varun Gupta in their paper [3] conduct an overview of dynamic software metrics. The later shows that software metrics have gone further from the field of static analysis and moved on to dynamic properties of the software.

\subsection{Source lines of code (SLOC) / lines of code (LOC)}
\label{background-source-lines-of-code}
\qquad Source lines of code (SLOC) or lines of code (LOC) is one of the most widely used, well-known and probably one of the oldest software source code metrics to date. As its name implies, SLOC is measured by counting the number of source codelines in order to give approximate estimation to software size and the total amount of efforts (man-hours) required for development, maintenance, etc. Usually comparisons involve only the order of magnitude of lines of code in the projects. An apparent disadvantage of SLOC metric is that its magnitude on the piece of software does not necessarily correlate with the functionality provided by that piece. SLOC values differ from one language to another and heavily depend on the source code formatting and stylistic factors. Despite all of its disadvantages, SLOC is widely used in software projects size estimations and generally gives good correlations between its magnitude and programming efforts.

\subsection{McCabe's cyclomatic complexity (CC)}
\label{background-cyclomatic-complexity}
\qquad Another well-known software metric is cyclomatic complexity (CC). The metric was first developed by Thomas J. McCabe in 1976 \cite{cyclomatic-complexity-paper}. The metric is based on the control flow graph (CFG) of the section of the code and basically represents the number of linearly independent paths through that section. Mathematically cyclomatic complexity M of a section of the code is defined as M = E – N + 2P, where E is the number of edges, N is the number of nodes, P is the number of connected components in the section's CFG. For example, the piece of code, which CFG is presented on the Figure 1, has cyclomatic complexity equal to 3. The same value 3 follows form it's mathematical equation M = 8 – 7 + 2 = 3. CC metric has been validated both empirically and theoretically and has a lot of applications.

\subsection{Halstead's complexity measures}
\label{background-halsteads-measures}
\qquad Maurice Halstead introduced his software science in 1977 \cite{halstead-book}. In his work Halstead built an analogy between measurable properties of matter (such as volume, mass and pressure of a gas) and those of a source code. He introduced such notions as program length, program volume and program difficulty based on the number of distinct operands and operators in the program.

\subsection{Software cohesion and coupling}
\label{background-cohesion-and-coupling}
\qquad Concepts of software coupling and cohesion were introduced into computer science by Larry Constantine in the late 1960s, when he was working on the field of structured design. The work \cite{cohesion-coupling-paper}, published in 1974 outlines the main results of Larry Constantine's research. Coupling is the degree of interdependence between software modules, while cohesion refers to the degree to which the elements inside the module belong together. These concepts are usually contrasted to each other and often establish inverse proportionality: high coupling often correlates with low cohesion and vice versa. Low coupling and high cohesion are usually a sign of a well-designed system. That system consists of the relatively independent modules. Changes in one part do not usually affect another parts. Degree of reusability is high and particular
system parts (obsolete, malfunctioning, etc.) can be replaced without affecting the rest of the system.

\subsection{Function points}
\qquad Function point is a “unit of measurement” that is used in order to represent the amount of business functionality present in the piece of software. During functional requirements phase of software development, required functionality is identified. Every function is categorized into one of the following types: output, input, inquiry, internal files and external interfaces. Every function is given some amount of function points, which is based on the experience of the past projects. Function Points were proposed by Allan Albrecht in 1979 [7]. Albrecht observed in his research that Function Points were highly correlated to SLOC (3.1) metric.

\subsection{Object-Oriented software metrics}
\qquad In the work [8] Chidamber and Kemerer define a suite of metrics for object oriented designs. They define software metrics for several software properties like cohesion, coupling and complexity. Some examples are presented below:
- Lack of Cohesion in Methods (LCOM): LCOM = (P > Q) ? P – Q : 0, where P
and Q are the numbers of pairs of class methods that do not use / use common class member variables correspondingly.
- Coupling Between Object Classes (CBO): for a class CBO equals to the number
of other classes to which it is coupled. If methods of a class invoke methods or work with member variables of the other class, then classes are coupled.

\subsection{Security metrics for source code structures}
\qquad Software metrics have found their application in the field of source code security as well. Work [9] gives some examples. Described metrics can be used at different stages of software development. Function points (3.5) can be used at initial stages of functional requirements specification. Software cohesion and coupling concepts (3.4) can be considered during later stages of high-level design specification (particular object-oriented software metrics (3.6)). Cyclomatic complexity (3.2), SLOC (3.1), Halstead's complexity measures (3.3) can be used during final and implementation stages for guiding coding efforts. All these metrics give assessments and predictions related to software quality, maintenance, testability, etc. Despite the possibility of correlations between some of these metrics and application parallelisability, these are not designed to directly judge about it.

\section{Metrics in the area of parallel computing}
\label{background-metrics-parallel-computing}
\qquad As in the whole computer science field, there have been proposals of numerous performance metrics in the area of parallel computing as well. Work \cite{parallel-performance-metrics-paper} gives a critical overview of some of the existent parallel performance metrics. These metrics assess software/architecture combinations and use a program running time as their basis. Subsection \ref{background-metrics-speedup-variants} takes some fragments of the work \cite{parallel-performance-metrics-paper} to give a reader an impression of available parallel performance metrics. References to original authors of these metrics are available in the work \cite{parallel-performance-metrics-paper}.     

\subsection{Speedup variants}
\label{background-metrics-speedup-variants}
\qquad The basic question, which arises with program parallelization is "How much faster are we running application on a parallel computer?". The metrics described in this subsection are motivated by that question. While there is a general agreement that a speedup is the ratio:
\begin{equation}
\frac{serial\; execution\; time}{parallel\; execution\; time}
\label{basic-speedup}
\end{equation}
there is a diversity in definitions of parallel and serial execution times.
\begin{enumerate}[align=left,leftmargin=*]
\item \qquad When we use the notion of \textit{relative speedup}, parallel execution time is the time needed to execute a parallel version of a program on a single processor. The final resulting speedup depends on many factors: the number of processors in the system, the interconnection topology used for processor communication, the input dataset for the program, etc. Hence, the final speedup numbers might differ and we may futher introduce a number of such metrics as \textit{maximum relative speedup}, \textit{minimum relative speedup}, etc. 
\item \qquad When we talk about \textit{real speedup}, the role of the \textit{serial execution time} is performed by the time, needed for the fastest known serial algorithm to solve the problem.   
\item \qquad In yet another speedup definition the serial execution time is measured on the fastest serial computer, executing the best known algorithm. The term \textit{absolute speedup} is used for this measure. 
\item \qquad Let $t_{serial}(n)$ and $t_{parallel}(n)$ be asymptotic complexities of a serial and parallel algorithms used to solve a problem respectively. Then the ratio $\frac{t_{serial}(n)}{t_{parallel}(n)}$ is called an \textit{asymptotic speedup}. Asymptotic speedup assumes unlimited number of available processors and is not a function of the number of processors in a system. As with regular speedup, this speedup can futher be classified into relative, real, etc.
\item \qquad If we introduce different parameters (such as hard drive read/write rate, memory latency L, number of processors P, processor cache sizes S, etc.) and write down a final equation for a speedup, then we get so-called \textit{analytical speedup}. 

\end{enumerate} 
\qquad All these enumerated speedup variants can be futher combined or modified to produce a numerous parallel performance metrics. While these metrics can be used to estimate the final speedup a parallel version of a program is going to have on the certain hardware system, they cannot be used for software source code parallelisability feedback and algorithmic parallelizability analysis. In other words, these metrics are not applicable to the problem being tackled in this MSc project.  

\section{Modern parallelisability advisor tools}
\label{background-modern-parallelisability-advisor-tools}
\subsection{Intel(R) Parallel Studio XE 2018}
\qquad Whithin the current project boundaries the tool is used in conjunction with Intel(R) Parallel Studio XE 2018 \cite{intel-parallel-studio}. Intel Parallel Studio XE is a software development product developed by Intel. Parallel Studio is composed of several component parts, each of which is a collection of capabilities. 

These tools help developers boost application performance through superior optimizations and Single Instruction Multiple Data (SIMD) vectorization, integration with Intel® Performance Libraries, and by leveraging the latest OpenMP* 5.0 parallel programming models.

Enhanced optimization reports and integration with Intel® VTune™ Amplifier and Intel® Advisor give developers control over code profiles.

For better performance, it is optimized to take advantage of advanced processor features like multiple cores and wider vector registers, including Intel® Advanced Vector Extensions 512 (Intel® AVX-512) instructions. 

Intel® C++ Compiler in Intel® Parallel Studio XE

\subsection{Automatic parallelisation with Intel(R) C/C++ compilers (ICC)}
\qquad Parallelizing application for the sake of performance improvement can be a time-consuming and skill-requiring activity. For applications, containing relatively simple loops and targeting x86 platforms this task can be automated with the help of Intel C++ compiler \cite{intel-multithreading-guide}. With automatic parallelization ICC detects loops that can be safely and efficiently parallelized and generates multithreaded code. It relieves the programmer from searching for loops that are good candidates for parallel execution, performing dependence analysis and adding parallel compiler directives manually. \newline \null\qquad When it comes to automatic program parallelisation, Intel C/C++ compilers are apparently limited to certain types of loops. \newline \null\qquad Along with actual parallelization Intel C/C++ compilers provide developers with a comprehensive parallelisation reports. \newline \null\qquad Intel C/C++ compiler is used withing the scope and timeframe of the current MSc project as loop parallelisation expert. It's parallelisability reports are transformed into the following format, shown in figure below. That data is used for later statistical learning analysis as labels and parallelisability classifications for different loops of NAS benchmarks (see chapter \ref{benchmarks}).      
   

\section{Dependence theory} \label{background-dependence-theory}
\qquad Modern optimizing and parallelizing compilers use dependence-based approaches to the analyses and transformations they do. Data dependence has been explored since the early days of compilers, dating back to the 1960s, and by now there exist a vast body or research and theory in the domain. The main results and outlines can be found in the optimizing compilers for modern architectures book \cite{optimizing-compilers-book}. Here the brief descriptions fo notions are provided.  
\subsection{Types of dependencies} \label{background-dependence}
\qquad Generally speaking, a dependence is anything that introduces execution order constraints on statements or instructions of the sequential program. Statement S2 is dependent on statement S1, if statement S1 must be executed before statement S2. Dependencies may be broadly classified into two different categories: data and control dependencies. If statement S2 consumes the data, produced by S1, then this type of dependence is called data dependence. If whether S2 will be executed or not depends on the outcome of computation done in S1, then the statement S2 is control-dependent on statement S1. \newline  
\null\qquad Data dependencies are futher subdivided into four subcategories.

\begin{description}
\item [Read After Write (RAW) dependencies]  
\item [Write After Read (WAR) dependencies]    
\item [Write After Write (WAW) dependencies] 
\item [Read After Read (RAR) dependencies] 
\end{description}

\section{Graph theory} \label{background-graph-theory}
\qquad The work uses some results from the graph theory. In particular, the depth-first search (DFS) graph traversal algorithm and its application to find strongly connected components (SCCs) of graphs. While there are a certain number of variations of these two basic algorithms, the work uses them in the exact form as described in the introduction to algorithms book \cite{introduction-to-algorithms-book}.

\section{Control flow analysis} \label{background-control-flow-analysis}
\qquad Control flow analysis \cite{advanced-compiler-design-book}

\section{Program Dependence Graph (PDG)} \label{background-program-dependence-graph}
\qquad A lot of work has been performed over the years in the area of dependence-based program representations and a lot of different  \newline 
\null\qquad The Program Dependence Graph (PDG) is an intermediate dependence-based program representation that makes explicit both the data and control dependencies for each operation in a program. A control flow graph [l, 31 has
been the usual representation for the control flow relationships of a program; the control conditions on which an operation depends can be derived from such a
graph. An undesirable property of a control flow graph, however, is a fixed
sequencing of operations that need not hold. The program dependence graph
explicitly represents both the essential data relationships, as present in the data dependence graph, and the essential control relationships, without the unnecessary sequencing present in the control flow graph.’ These dependence relationships determine the necessary sequencing between operations, exposing potential parallelism. 




\subsection{Data dependence graph (DDG)} \label{background-ddg}
\subsection{Memory dependence graph (MDG)} \label{background-mdg}
\subsection{Control dependence graph (CDG)} \label{background-cdg}
\subsection{Program dependence graph (PDG)} \label{background-pdg}

\section{Loop iterator recognition and loop decoupling} \label{background-loop-decoupling}
\qquad Logically the code, constituting a loop can be divided into two parts. The first part is an actual workload (payload) to be repeated multiple times. The other part is loop iterator the code to control the repetition of the workload.    

\cite{iterator-recognition-paper}

