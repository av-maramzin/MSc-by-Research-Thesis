\chapter{Introduction}
\label{introduction}
\section{General words on parallel programming}
\qquad Parallelism pervades the modern computing world. In the past parallel computations used to be employed only in high performance scientific systems, but now the situation has changed. Parallel elements present in the design of almost all modern computers from small embedded processors to large-scale supercomputers and computing networks. Unfortunately, these immense parallel computing hardware resources are not always fully utilized during computations due to several problems in the field:

\begin{enumerate}[align=left,leftmargin=*]
\item \textbf{Abundance of legacy applications from previous sequential computing era.} That abundance is one source of problems. Legacy applications are not designed to run on parallel machines and, by default, do not take advantage of all underlying hardware resources. Automatic parallelisation techniques have been developed to transform these sequential applications into parallel ones. However, these techniques cannot efficiently deal with some codes in the spectrum of existent applications. Some simple examples of such codes can be found in sections \ref{background-modern-parallelisability-advisor-tools} and \ref{analysis-manual-analysis} of the thesis. More complex codes like pointer-based applications with irregular data structures, applications with loop carried dependencies and entangled control flow have proven to be even more challenging to automatic parallelisation. Very often such programs hide significant amounts of parallelism behind suboptimal implementation constructs and represent meaningful potential for further improvements.
	
\item \textbf{Difficulty of manual parallel programming.} Hidden potential can be realised by writing parallel programs (applications designed to run on parallel systems) manually. However, the task of manual parallel programming is rather challenging by itself. To create efficient and well-designed parallel software programmer must be aware of application's domain field, must have good algorithmic background as well as solid general programming skills and working knowledge of exact parallel programming framework they are using. Most “average” programmers lack some of the necessary skills out of that set, which hinders the potential of manual parallelisation. Sometimes sloppy program parallelisation can even slow sequential programs down due to parallel synchronisation/communication overhead incurred.

\end{enumerate}

\section{Problems of modern parallelizability assistance tools}
\null\qquad While there are some available tools for programmer assistance in software parallelization (like Intel Parallel Studio \ref{background-modern-parallelisability-advisor-tools}), these tools are quite complex and require acquisition of relevant expertise, before they can be effectively used by a programmer. Intel parallelizing compiler (see section \ref{background-modern-parallelisability-advisor-tools}) provides some parallelizability reports to a user. However, these reports mostly have a binary nature: they give "yes"/"no" answers to a parallelizability question. Moreover, Intel compiler has to be conservative in its optimizations in order to preserve program's functional semantic. Sometimes it is too conservative, which leads to detection of different sorts of dependencies in algorithmically parallel loops. In situations, where a programmer chose suboptimal implementations (linked-list instead of a linear array for example) without legitimate reason, Intel compiler will report "a loop is not a candidate for parallelization". More detailed reports would be useful in such situations. Sections \ref{background-modern-parallelisability-advisor-tools} and \ref{analysis-manual-analysis} give some examples of such cases.\newline
\section{Software source code metrics for parallelism}
\qquad In our project we propose to research the question of software metrics for parallelism. This research idea draws on the existent work in the area of \textit{software quality}, where numerous software metrics have been proposed. The most illustrative example of these is the \textit{cyclomatic complexity} measure of a section of a code. In short, cyclomatic complexity is the number of independent paths through control-flow graph of a section of a code. This metric reflects complexity of a source code, and, hence, such properties as readability, maintainability, testability, etc.\newline 
\null\qquad Section \ref{background-software-metrics-in-cs} gives an overview of existent metrics in the general field of computer science and some major related work. The major problem of these metrics is that they do not always correlate with software quality properties and cannot be used blindly alone to judge about those.\newline 
\null\qquad The major questions of this research work are: "Whether these metrics can be adjusted to judge about software parallelizability property?", "Whether there are any metrics, devised for tackling software parallelizability problem?" and "What metrics would be the best for judging about software source code parallelizability?".\newline
\null\qquad As it became clear after related work overview, there hadn't been proposed any software metrics for parallelism. Available metrics for parallelism represent different variations of parallel program \textit{speedup} relative to its sequential version. Section \ref{background-metrics-parallel-computing} presents an overview of related work on that topic.\newline

\section{Software metrics for parallelism as a simple quantitative feedback to a programmer}
\null\qquad The goal of this project is to research the possibility of provision of a more detailed parallelization feedback to a programmer. Instead of binary "yes"/"no" answers, ideally, we would like to see a feedback like: this code is "10\% parallelizible", "80\% parallelizible" or "100\% parallelizible". Where 100\% would mean that this code can be parallelized straight away. Code, which has been assessed as only 10\% is  suboptimal and requires a great deal of work to make it parallel, if it is possible at all. While 80\% parallelizible code requires a small change to turn it into 100\% parallel implementation.\newline
\null\qquad Let's consider two code snippets shown in listings \ref{lst:motivation-0} and \ref{lst:motivation-1}. Both of these code fragments implement the same simple and parallelizible algorithm. While implementation \ref{lst:motivation-0} uses linear array, implementation \ref{lst:motivation-1} uses suboptimal data structure (for that particular task) - linked-list. Intel compiler will give two answers: "yes" for \ref{lst:motivation-0} and "this loop is not a parallel candidate" for \ref{lst:motivation-1}. Here we would like our metrics to provide a programmer with a hint: "\textit{Code in \ref{lst:motivation-1} is not parallelizible, but the algorithm is, and if you rewrite the code, you can run it in parallel}".   

\begin{lstlisting}[float,floatplacement=H,caption={Parallelizible algorithm is implemented with a parallelizible code - parallelizibility metric would report 100\% parallelizible.}, captionpos=b, label=lst:motivation-0]
for (i = 0; i < n; i++) {
	a[i] = a[i] + 1;
}
\end{lstlisting}

\begin{lstlisting}[float,floatplacement=H,caption={Parallelizible algorithm ends up hidden behind non-parallelizible construct (pointer chasing code) - metric would give, say, $\approx$ 20\% parallelizible - there is a great deal of work to do, before it can be turned into a parallel code, but it is possible.}, captionpos=b, label=lst:motivation-1]
while (ptr != nullptr) {
	ptr->value++;
	ptr = ptr->next;
}
\end{lstlisting}      

\section{The structure and the content of the thesis}
\qquad Chapter \ref{backgroud} presents an overview of related work, specifically it describes different software metrics, proposed in the general field of computer science \ref{background-software-metrics-in-cs}, as well as in the subfield of parallel programming \ref{background-metrics-parallel-computing}. It also introduces a reader into the context of the work by giving a necessary background. The major works this thesis is based on are Program Dependence Graph (PDG) intermediate representation proposal \cite{pdg-paper} (described in section \ref{background-program-dependence-graph}) and loop decoupling and iterator recognition work \cite{iterator-recognition-paper} (described in section \ref{background-loop-decoupling}). Metrics, devised and proposed in this work rely heavily on dependence analysis theory \cite{optimizing-compilers-book} (described in section \ref{background-dependence-theory}) and are of dependence-based nature.\newline
\null\qquad Chapter \ref{metrics} introduces actual metrics devised and proposed in this work. It motivates their proposal with some intuitive considerations, gives their definitions and describes how one would use them.\newline
\null\qquad Chapter \ref{ppar-tool} describes PPar (Pervasive Parallelism) tool, developed withing that project. The tool is based on the LLVM compiler components library \cite{llvm-paper}, \cite{llvm-official-website} and computes all proposed metrics on Program Dependence Graphs (PDGs) of program loops. Along with metrics computation it provides graph visualization facilities (see subsection \ref{ppar-tool-graph-visualizations}), which can be used for tool debugging and metrics analysis.\newline
\null\qquad Chapter \ref{benchmarks} describes NAS parallel benchmarks used withing that project for parallelizability metrics research.\newline
\null\qquad Chapter \ref{analysis} presents the evalueation of metrics. It shows different plots of single metric values against software parallelizability property (see section \ref{analysis-data-interpretation-and-visualization}), which allow to see all metrics, correlating with software parallelizability. The chapter also contains clustering analysis (see section \ref{analysis-data-clustering-analysis}), which considers all metrics altogether, as well as decision trees, which represent parallelizability classification derivation based on metric values (see subsection \ref{analysis-decision-tree}). Visualizations of metric values are supplemented with some manual insights into the source code of benchmarks (see subsection \ref{analysis-manual-analysis}). Chapter concludes with application of different machine learning (ML) techniques in order to see which metrics (ML features in that context) are the best for parallelizability ML model training (see section \ref{analysis-statistical-analysis}). 